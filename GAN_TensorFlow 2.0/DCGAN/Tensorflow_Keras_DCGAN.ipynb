{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "Tensorflow_Keras_DCGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3.6 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgMWBuf61OmL"
      },
      "source": [
        "# Nicely formatted time string\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mla8_jr2HnbV",
        "outputId": "33b1c35f-9803-4043-bb08-9b71fe3ac80e"
      },
      "source": [
        "hms_string(700)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0:11:40.00'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yqlUD4sdiDk"
      },
      "source": [
        "#Implementing DCGANs in Keras\n",
        "\n",
        "Paper that described the type of DCGAN that we will create in this module. [[Cite:radford2015unsupervised]](https://arxiv.org/abs/1511.06434) This paper implements a DCGAN as follows:\n",
        "\n",
        "* No pre-processing was applied to training images besides scaling to the range of the tanh activation function [-1, 1]. \n",
        "* All models were trained with mini-batch stochastic gradient descent (SGD) with a mini-batch size of 128. \n",
        "* All weights were initialized from a zero-centered Normal distribution with standard deviation 0.02. \n",
        "* In the LeakyReLU, the slope of the leak was set to 0.2 in all models.\n",
        "* we used the Adam optimizer(Kingma & Ba, 2014) with tuned hyperparameters. We found the suggested learning rate of 0.001, to be too high, using 0.0002 instead. \n",
        "* Additionally, we found leaving the momentum term $\\beta{1}$ at the suggested value of 0.9 resulted in training oscillation and instability while reducing it to 0.5 helped stabilize training.\n",
        "\n",
        "The paper also provides the following architecture guidelines for stable Deep Convolutional GANs:\n",
        "\n",
        "* Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator).\n",
        "* Use batchnorm in both the generator and the discriminator.\n",
        "* Remove fully connected hidden layers for deeper architectures.\n",
        "* Use ReLU activation in generator for all layers except for the output, which uses Tanh.\n",
        "* Use LeakyReLU activation in the discriminator for all layers.\n",
        "\n",
        "While creating the material for this module I used a number of Internet resources, some of the most helpful were:\n",
        "\n",
        "* [Deep Convolutional Generative Adversarial Network (TensorFlow 2.0 example code)](https://www.tensorflow.org/tutorials/generative/dcgan)\n",
        "* [Keep Calm and train a GAN. Pitfalls and Tips on training Generative Adversarial Networks](https://medium.com/@utk.is.here/keep-calm-and-train-a-gan-pitfalls-and-tips-on-training-generative-adversarial-networks-edd529764aa9)\n",
        "* [Collection of Keras implementations of Generative Adversarial Networks GANs](https://github.com/eriklindernoren/Keras-GAN)\n",
        "* [dcgan-facegenerator](https://github.com/platonovsimeon/dcgan-facegenerator), [Semi-Paywalled Article by GitHub Author](https://medium.com/datadriveninvestor/generating-human-faces-with-keras-3ccd54c17f16)\n",
        "\n",
        "The program created next will generate faces similar to these.  While these faces are not perfect, they demonstrate how we can construct and train a GAN on or own.  Later we will see how to import very advanced weights from nVidia to produce high resolution, realistic looking faces. Figure 7.GAN-GRID shows images from GAN training.\n",
        "\n",
        "**Figure 7.GAN-GRID: GAN Neural Network Training**\n",
        "![GAN](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan-3.png \"GAN Images\")\n",
        "\n",
        "GAN is made up of two different neural networks: the discriminator and the generator.  The generator generates the images, while the discriminator detects if a face is real or was generated.  These two neural networks work as shown in Figure 7.GAN-EVAL:\n",
        "\n",
        "**Figure 7.GAN-EVAL: Evaluating GANs**\n",
        "![GAN](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan_fig_1.png \"GAN\")\n",
        "\n",
        "The discriminator accepts an image as its input and produces number that is the probability of the input image being real.  The generator accepts a random seed vector and generates an image from that random vector seed. An unlimited number of new images can be created by providing additional seeds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpCjlQyEdiDo"
      },
      "source": [
        "I suggest running this code with a GPU, it will be very slow on a CPU alone.  The following code mounts your Google drive for use with Google CoLab.  If you are not using CoLab, the following code will not work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Y8_-1h5ddiDp",
        "outputId": "61ff3ad9-1766-457a-f65d-e2c86d00f012"
      },
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Note: using Google CoLab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeVAWGHOdiDl"
      },
      "source": [
        "The following packages will be used to implement a basic GAN system in Python/Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KubxTY1mdiDm"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Reshape, Dropout, Dense \n",
        "from tensorflow.keras.layers import Flatten, BatchNormalization\n",
        "from tensorflow.keras.layers import Activation, ZeroPadding2D\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import os \n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9uwIRYJdiDr"
      },
      "source": [
        "These are the constants that define how the GANs will be created for this example.  The higher the resolution, the more memory that will be needed.  Higher resolution will also result in longer run times.  For Google CoLab (with GPU) 128x128 resolution is as high as can be used (due to memory).  Note that the resolution is specified as a multiple of 32.  So **GENERATE_RES** of 1 is 32, 2 is 64, etc.\n",
        "\n",
        "To run this you will need training data.  The training data can be any collection of images.  I suggest using training data from the following two locations.  Simply unzip and combine to a common directory.  This directory should be uploaded to Google Drive (if you are using CoLab). The constant **DATA_PATH** defines where these images are stored.\n",
        "\n",
        "The source data (faces) used in this module can be found here:\n",
        "\n",
        "* [Kaggle Faces Data New](https://www.kaggle.com/gasgallo/faces-data-new)\n",
        "* [Kaggle Lag Dataset: Dataset of faces, from more than 1k different subjects](https://www.kaggle.com/gasgallo/lag-dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tb_XblE7diDr",
        "outputId": "66c92312-f505-4702-8f76-2a9adfd7d637"
      },
      "source": [
        "# Generation resolution - Must be square \n",
        "# Training data is also scaled to this.\n",
        "# Note GENERATE_RES 4 or higher  \n",
        "# will blow Google CoLab's memory and have not\n",
        "# been tested extensivly.\n",
        "GENERATE_RES = 3 # Generation resolution factor \n",
        "# (1=32, 2=64, 3=96, 4=128, etc.)\n",
        "GENERATE_SQUARE = 32 * GENERATE_RES # rows/cols (should be square)\n",
        "IMAGE_CHANNELS = 3\n",
        "\n",
        "# Preview image \n",
        "PREVIEW_ROWS = 4\n",
        "PREVIEW_COLS = 7\n",
        "PREVIEW_MARGIN = 16\n",
        "\n",
        "# Size vector to generate images from\n",
        "SEED_SIZE = 100\n",
        "\n",
        "# Configuration\n",
        "DATA_PATH = '/content/drive/MyDrive/TDI/VanillaGANs/faces'\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 60000\n",
        "\n",
        "print(f\"Will generate {GENERATE_SQUARE}px square images.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Will generate 96px square images.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDTfFQjTdiDu"
      },
      "source": [
        "Next we will load and preprocess the images.  This can take a while.  Google CoLab took around an hour to process.  Because of this we store the processed file as a binary.  This way we can simply reload the processed training data and quickly use it.  It is most efficient to only perform this operation once.  The dimensions of the image are encoded into the filename of the binary file because we need to regenerate it if these change."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ69ALfSdiDv"
      },
      "source": [
        "# Image set has 11,682 images.  Can take over an hour \n",
        "# for initial preprocessing.\n",
        "# Because of this time needed, save a Numpy preprocessed file.\n",
        "# Note, that file is large enough to cause problems for \n",
        "# sume verisons of Pickle,\n",
        "# so Numpy binary files are used.\n",
        "training_binary_path = os.path.join(DATA_PATH,\n",
        "        f'training_data_{GENERATE_SQUARE}_{GENERATE_SQUARE}.npy')\n",
        "\n",
        "print(f\"Looking for file: {training_binary_path}\")\n",
        "\n",
        "if not os.path.isfile(training_binary_path):\n",
        "  start = time.time()\n",
        "  print(\"Loading training images...\")\n",
        "\n",
        "  training_data = []\n",
        "  faces_path = os.path.join(DATA_PATH,'images')\n",
        "  for filename in tqdm(os.listdir(faces_path)):\n",
        "      path = os.path.join(faces_path,filename)\n",
        "      image = Image.open(path).resize((GENERATE_SQUARE,\n",
        "            GENERATE_SQUARE),Image.ANTIALIAS)\n",
        "      training_data.append(np.asarray(image))\n",
        "  training_data = np.reshape(training_data,(-1,GENERATE_SQUARE,\n",
        "            GENERATE_SQUARE,IMAGE_CHANNELS))\n",
        "  training_data = training_data.astype(np.float32)\n",
        "  training_data = training_data / 127.5 - 1.\n",
        "\n",
        "\n",
        "  print(\"Saving training image binary...\")\n",
        "  np.save(training_binary_path,training_data)\n",
        "  elapsed = time.time()-start\n",
        "  print (f'Image preprocess time: {hms_string(elapsed)}')\n",
        "else:\n",
        "  print(\"Loading previous training pickle...\")\n",
        "  training_data = np.load(training_binary_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9kO_iSRyixQ"
      },
      "source": [
        "We will use a TensorFlow **Dataset** object to actually hold the images.  This allows the data to be quickly shuffled int divided into the appropriate batch sizes for training.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BXl0JohJBx69"
      },
      "source": [
        "# Batch and shuffle the data\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(training_data) \\\n",
        "    .shuffle(BUFFER_SIZE).batch(BATCH_SIZE)       #BUFFER_SIZE: representing the number of elements from this dataset from which the new dataset will be sampled."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dATyXqQdiDw"
      },
      "source": [
        "The code below creates the generator and discriminator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB_aX4ChdiD0"
      },
      "source": [
        "Next we actually build the discriminator and the generator.  Both will be trained with the Adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ulou-BZPybzT"
      },
      "source": [
        "def build_generator(seed_size, channels):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(4*4*256,activation=\"relu\",input_dim=seed_size))\n",
        "    model.add(Reshape((4,4,256)))\n",
        "\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "   \n",
        "    # Output resolution, additional upsampling\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    if GENERATE_RES>1:\n",
        "      model.add(UpSampling2D(size=(GENERATE_RES,GENERATE_RES)))\n",
        "      model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n",
        "      model.add(BatchNormalization(momentum=0.8))\n",
        "      model.add(Activation(\"relu\"))\n",
        "\n",
        "    # Final CNN layer\n",
        "    model.add(Conv2D(channels,kernel_size=3,padding=\"same\"))\n",
        "    model.add(Activation(\"tanh\"))\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E8zpCLNzqDM4",
        "outputId": "d7c6cf3e-4469-44bc-d2ec-09fb2c1dddff"
      },
      "source": [
        "#Summary of Generator Model\r\n",
        "model = build_generator(SEED_SIZE, 3)\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 4096)              413696    \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d (UpSampling2D) (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 16, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16, 16, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 128)       295040    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 32, 32, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2 (None, 96, 96, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 96, 96, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 96, 96, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 96, 96, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 96, 96, 3)         3459      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 96, 96, 3)         0         \n",
            "=================================================================\n",
            "Total params: 2,043,011\n",
            "Trainable params: 2,041,475\n",
            "Non-trainable params: 1,536\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iWR5xSKUUB-k"
      },
      "source": [
        "def build_discriminator(image_shape):\r\n",
        "    model = Sequential()\r\n",
        "\r\n",
        "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=image_shape, \r\n",
        "                     padding=\"same\"))\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "    model.add(Dropout(0.25))\r\n",
        "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\r\n",
        "    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\r\n",
        "    model.add(BatchNormalization(momentum=0.8))\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "    model.add(Dropout(0.25))\r\n",
        "    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\r\n",
        "    model.add(BatchNormalization(momentum=0.8))\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "    model.add(Dropout(0.25))\r\n",
        "    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\r\n",
        "    model.add(BatchNormalization(momentum=0.8))\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "    model.add(Dropout(0.25))\r\n",
        "    model.add(Conv2D(512, kernel_size=3, strides=1, padding=\"same\"))\r\n",
        "    model.add(BatchNormalization(momentum=0.8))\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "    model.add(Dropout(0.25))\r\n",
        "    model.add(Flatten())\r\n",
        "    model.add(Dense(1, activation='sigmoid'))\r\n",
        "  \r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JoqBRKV6p-1J",
        "outputId": "e6adc048-93e6-4705-c12c-1b0a397f5511"
      },
      "source": [
        "#ModelSummary of Discriminator\r\n",
        "image_shape = (GENERATE_SQUARE,GENERATE_SQUARE,IMAGE_CHANNELS)\r\n",
        "Model = build_discriminator(image_shape)\r\n",
        "Model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 48, 48, 32)        896       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 48, 48, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 48, 48, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2 (None, 25, 25, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 25, 25, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 25, 25, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 25, 25, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 13, 13, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 13, 13, 128)       512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 13, 13, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 13, 13, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 13, 13, 256)       295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 13, 13, 256)       1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 13, 13, 256)       0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 13, 13, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 13, 13, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 13, 13, 512)       2048      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 13, 13, 512)       0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 13, 13, 512)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 86528)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 86529     \n",
            "=================================================================\n",
            "Total params: 1,658,945\n",
            "Trainable params: 1,657,025\n",
            "Non-trainable params: 1,920\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kxKk7uDOnPS"
      },
      "source": [
        "As we progress through training images will be produced to show the progress.  These images will contain a number of rendered faces that show how good the generator has become.  These faces will be "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UKnCeDut2cp0"
      },
      "source": [
        "def save_images(cnt,noise):\n",
        "  image_array = np.full(( \n",
        "      PREVIEW_MARGIN + (PREVIEW_ROWS * (GENERATE_SQUARE + PREVIEW_MARGIN)), \n",
        "      PREVIEW_MARGIN + (PREVIEW_COLS * (GENERATE_SQUARE+PREVIEW_MARGIN)), 3), \n",
        "      255, dtype=np.uint8)\n",
        "  \n",
        "  generated_images = generator.predict(noise)\n",
        "\n",
        "  generated_images = generated_images    #0.5 * generated_images + 0.5\n",
        "\n",
        "  image_count = 0\n",
        "  for row in range(PREVIEW_ROWS):\n",
        "      for col in range(PREVIEW_COLS):\n",
        "        r = row * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n",
        "        c = col * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n",
        "        image_array[r:r+GENERATE_SQUARE,c:c+GENERATE_SQUARE] \\\n",
        "            = generated_images[image_count] * 255\n",
        "        image_count += 1\n",
        "\n",
        "          \n",
        "  output_path = os.path.join(DATA_PATH,'output_vanillaGAN')\n",
        "  if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)\n",
        "  \n",
        "  filename = os.path.join(output_path,f\"train-{cnt}.png\")\n",
        "  im = Image.fromarray(image_array)\n",
        "  im.save(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWIPw2fHu3ky"
      },
      "source": [
        "**Generate an image from random noise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL5byGhNzOzd"
      },
      "source": [
        "generator = build_generator(SEED_SIZE, IMAGE_CHANNELS)\n",
        "\n",
        "noise = tf.random.normal([1, SEED_SIZE])\n",
        "generated_image = generator(noise, training=False)\n",
        "\n",
        "plt.imshow(generated_image[0, :, :, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlxxEHDIOqjW"
      },
      "source": [
        "**Probability value of randomly generated image by Discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOnTxIXnyeEQ"
      },
      "source": [
        "image_shape = (GENERATE_SQUARE,GENERATE_SQUARE,IMAGE_CHANNELS)\n",
        "\n",
        "discriminator = build_discriminator(image_shape)\n",
        "decision = discriminator(generated_image)\n",
        "print (decision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ChOo3D1OsVc"
      },
      "source": [
        "Loss functions must be developed that allow the generator and discriminator to be trained in an adversarial way.  Because these two neural networks are being trained independently they must be trained in two separate passes.  This requires two separate loss functions and also two separate updates to the gradients.  When the discriminator's gradients are applied to decrease the discriminator's loss it is important that only the discriminator's weights are update.  It is not fair, nor will it produce good results, to adversarially damage the weights of the generator to help the discriminator.  A simple backpropagation would do this.  It would simultaneously affect the weights of both generator and discriminator to lower whatever loss it was assigned to lower.\n",
        "\n",
        "Figure 7.TDIS shows how the discriminator is trained.\n",
        "\n",
        "**Figure 7.TDIS: Training the Discriminator**\n",
        "![Training the Discriminator](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan_fig_2.png \"Training the Discriminator\")\n",
        "\n",
        "Here a training set is generated with an equal number of real and fake images.  The real images are randomly sampled (chosen) from the training data.  An equal number of random images are generated from random seeds.  For the discriminator training set, the $x$ contains the input images and the $y$ contains a value of 1 for real images and 0 for generated ones.\n",
        "\n",
        "Likewise, the Figure 7.TGEN shows how the generator is trained.\n",
        "\n",
        "**Figure 7.TGEN: Training the Generator**\n",
        "![Training the Generator](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan_fig_3.png \"Training the Generator\")\n",
        "\n",
        "For the generator training set, the $x$ contains the random seeds to generate images and the $y$ always contains the value of 1, because the optimal is for the generator to have generated such good images that the discriminiator was fooled into assigning them a probability near 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBaP98zAySJV"
      },
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIlZvHGAxbWf"
      },
      "source": [
        "Both the generator and discriminator use Adam and the same learning rate and momentum.  This does not need to be the case.  If you use a **GENERATE_RES** greater than 3 you may need to tune these learning rates, as well as other training and hyperparameters.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79UDhOCa0R4h"
      },
      "source": [
        "#Optimizer for Generator\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\n",
        "\n",
        "#Optimizer for Descriminator\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frCpCNn8yRcM"
      },
      "source": [
        "The following function is where most of the training takes place for both the discriminator and the generator.  This function was based on the GAN provided by the [TensorFlow Keras exmples](https://www.tensorflow.org/tutorials/generative/dcgan) documentation.  The first thing you should notice about this function is that it is annotated with the **tf.function** annotation.  This causes the function to be precompiled and improves performance.\n",
        "\n",
        "This function trans differently than the code we previously saw for training.  This code makes use of **GradientTape** to allow the discriminator and generator to be trained together, yet separately.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzyh-LqU0j5d"
      },
      "source": [
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "  seed = tf.random.normal([BATCH_SIZE, SEED_SIZE])  #no-of images to be generated is BATCH_SIZE\n",
        "\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    generated_images = generator(seed, training=True)\n",
        "\n",
        "    real_output = discriminator(images, training=True)\n",
        "    fake_output = discriminator(generated_images, training=True)\n",
        "    \n",
        "    gen_loss = generator_loss(fake_output)\n",
        "    disc_loss = discriminator_loss(real_output, fake_output)\n",
        "    \n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(\\\n",
        "        gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(\\\n",
        "        disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(\n",
        "        gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(\n",
        "        gradients_of_discriminator, \n",
        "        discriminator.trainable_variables))\n",
        "  return gen_loss,disc_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjrRgDR10lSF"
      },
      "source": [
        "def train(dataset, epochs):\n",
        "  fixed_seed = np.random.normal(0, 1, (PREVIEW_ROWS * PREVIEW_COLS, \n",
        "                                       SEED_SIZE))\n",
        "  start = time.time()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    gen_loss_list = []\n",
        "    disc_loss_list = []\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      t = train_step(image_batch)\n",
        "      gen_loss_list.append(t[0])\n",
        "      disc_loss_list.append(t[1])\n",
        "      print(\"Real output shape\", real_output.shape)\n",
        "      print(\"Fake_output shape\", fake_output.shape)\n",
        "\n",
        "    g_loss = sum(gen_loss_list) / len(gen_loss_list)\n",
        "    d_loss = sum(disc_loss_list) / len(disc_loss_list)\n",
        "\n",
        "    epoch_elapsed = time.time()-epoch_start\n",
        "    print (f'Epoch {epoch+1}, gen loss={g_loss},disc loss={d_loss},'\\\n",
        "           ' {hms_string(epoch_elapsed)}')\n",
        "    save_images(epoch,fixed_seed)\n",
        "\n",
        "  elapsed = time.time()-start\n",
        "  print (f'Training time: {hms_string(elapsed)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWmEHprD0t1V"
      },
      "source": [
        "train(train_dataset, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15Hia_feD9sm"
      },
      "source": [
        "generator.save(os.path.join(DATA_PATH,\"face_generator.h5\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btLk3mIIUXTQ"
      },
      "source": [
        "-----------------------------------------------"
      ]
    }
  ]
}